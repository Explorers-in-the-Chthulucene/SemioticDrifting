{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Extraction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the originality of the text we are dealing with, we decided to use several automated tools along with our analogical interpretation of the text. Our aims where, on the one hand, <b>achieving a comprehensive understanding of the content of the text on a conceptual level</b>, on the other hand <b>annotate the text syntactically and semantically</b> to be able to inquire deeper in the relation between Haraway's theory and the semiotics that convey it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "\n",
    "Along with the personal interpretation of our sources, we used <a href=\"https://github.com/MaartenGr/BERTopic\">BERTopic</a> to extract meaningful topics, especially the central theoretical concepts, from the text.\n",
    "\n",
    "<a href=\"\">find here</a> the documentation of this step as well as the visualization of the book's extracted topics."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition, POS tagging, dependency and more\n",
    "\n",
    "We used <a href=\"https://github.com/booknlp/booknlp\">Book NLP</a>, natural language processing pipeline that scales to books and other long documents (in English). To fully annotate the text and extract its main <b>entities</b> as well as their relations and dependencies.\n",
    "\n",
    "The pipeline produces 6 files: Tokens, Entities, Supersense, Quotes (tsv) and two Book files (json and html).\n",
    "\n",
    "Althought the entire output was useful for in-depth analysis of <i>Staying with the trouble</i> it didn't seem to perform very accurately on it, according to our needs, probably because of the philosophical and poetic content, in comparison to more narrative texts. \n",
    "\n",
    "For this reason we decided to concentrate only on one chapter, <b>Chapter 7: A Curious Practice</b> and manually correct the output of the <b>tokens</b> file. This process allowed us to precisely <a href=\"\">access the tool's accuracy</a> and to obtain <a href=\"\">useful data</a> for the creation of our ontology vocabulary and model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paragraph_ID</th>\n",
       "      <th>sentence_ID</th>\n",
       "      <th>token_ID_within_sentence</th>\n",
       "      <th>token_ID_within_document</th>\n",
       "      <th>word</th>\n",
       "      <th>lemma</th>\n",
       "      <th>byte_onset</th>\n",
       "      <th>byte_offset</th>\n",
       "      <th>POS_tag</th>\n",
       "      <th>fine_POS_tag</th>\n",
       "      <th>dependency_relation</th>\n",
       "      <th>syntactic_head_ID</th>\n",
       "      <th>event</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Vinciane</td>\n",
       "      <td>Vinciane</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>compound</td>\n",
       "      <td>1</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Despret</td>\n",
       "      <td>Despret</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>NNP</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>thinks</td>\n",
       "      <td>think</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>VERB</td>\n",
       "      <td>VBZ</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>HYPH</td>\n",
       "      <td>punct</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>with</td>\n",
       "      <td>with</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>ADP</td>\n",
       "      <td>IN</td>\n",
       "      <td>prep</td>\n",
       "      <td>2</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   paragraph_ID  sentence_ID  token_ID_within_sentence  \\\n",
       "0             0            0                         0   \n",
       "1             0            0                         1   \n",
       "2             0            0                         2   \n",
       "3             0            0                         3   \n",
       "4             0            0                         4   \n",
       "\n",
       "   token_ID_within_document      word     lemma  byte_onset  byte_offset  \\\n",
       "0                         0  Vinciane  Vinciane           0            8   \n",
       "1                         1   Despret   Despret           9           16   \n",
       "2                         2    thinks     think          17           23   \n",
       "3                         3         -         -          23           24   \n",
       "4                         4      with      with          24           28   \n",
       "\n",
       "  POS_tag fine_POS_tag dependency_relation  syntactic_head_ID event  \n",
       "0   PROPN          NNP            compound                  1     O  \n",
       "1   PROPN          NNP               nsubj                  2     O  \n",
       "2    VERB          VBZ                ROOT                  2     O  \n",
       "3   PUNCT         HYPH               punct                  2     O  \n",
       "4     ADP           IN                prep                  2     O  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "tokens_df = pd.read_csv(\"script\\BookNLP\\ch_7\\ch_7.tokens\", delimiter=\"\\t\")\n",
    "tokens_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COREF</th>\n",
       "      <th>start_token</th>\n",
       "      <th>end_token</th>\n",
       "      <th>prop</th>\n",
       "      <th>cat</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>PROP</td>\n",
       "      <td>PER</td>\n",
       "      <td>Vinciane Despret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>NOM</td>\n",
       "      <td>PER</td>\n",
       "      <td>other beings , human and not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>34</td>\n",
       "      <td>NOM</td>\n",
       "      <td>LOC</td>\n",
       "      <td>the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>47</td>\n",
       "      <td>PROP</td>\n",
       "      <td>PER</td>\n",
       "      <td>Despret</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>60</td>\n",
       "      <td>PRON</td>\n",
       "      <td>PER</td>\n",
       "      <td>her</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   COREF  start_token  end_token  prop  cat                          text\n",
       "0     14            0          1  PROP  PER              Vinciane Despret\n",
       "1     25            5         10   NOM  PER  other beings , human and not\n",
       "2     26           33         34   NOM  LOC                     the world\n",
       "3     14           47         47  PROP  PER                       Despret\n",
       "4     14           60         60  PRON  PER                           her"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_df = pd.read_csv(\"script\\BookNLP\\ch_7\\ch_7.entities\", delimiter=\"\\t\")\n",
    "entities_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FRED and FrameNet\n",
    "\n",
    "<a href=\"https://github.com/anuzzolese/fredclient\">FRED</a> is a machine reader for the Semantic Web: it is able to parse natural language text in 48 different languages and transform it to linked data.\n",
    "\n",
    "Our initial aim was to exploit FRED's output to add semantic role labeling of the entities we extracted in the previous steps. Unfortunately, this task resulted too time-consuming and complex to preform in the contest of this project, that's why we decided to try and mimic fred's semantic notation through a more simple, semi-automated process that implied manually annotating identified Frames and Roles in <b>the first paragraph</b> of Chapter 7 and creating a <b>Python pipeline</b> to automatically create instances of entities and their relation, according to our designed Ontology model.\n",
    "\n",
    "To annotate Frames we studied <a href=\"https://framenet.icsi.berkeley.edu/\">FrameNet project</a> and went through its lexical units index to see if we could classify our sentences in appropriate FrameNet frames. When suitable, we adopted FrameNet data, otherwise we created our own Frame, such as in the case of the Frame <b>Collaborative_thinking</b>, give that it evokes a semantic concept that is original to Donna Haraway's theory.\n",
    "\n",
    "the identification of frames in the text was carried out following this workflow:\n",
    "\n",
    "1. Identification of all the main concepts\n",
    "2. Individuation of the significant terms related to each concept and relation of concepts between them\n",
    "3. Abstraction into Frames evoked by the previous conceptualization\n",
    "4. Aligning identified Frames with FrameNet's where possible\n",
    "5. Listing of lexical units (concept-related identified lexical units + new FrameNet related lexical unit)\n",
    "\n",
    "Find <a href=\"\">here</a> all annotated Frames of our first paragraph."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
